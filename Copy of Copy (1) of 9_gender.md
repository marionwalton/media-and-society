---
layout: page
title: 10.2. Gender bias and AI
---

# Gender bias and AI
{: .no_toc }

*By Marion Walton & Martha Evans, University of Cape Town*

- TOC
{:toc}

GenerativeAI models tend to perpetuate the biases in its training data. For this reason, gender and other biases in synthetic media are a serious concern for feminist media researchers. Various kinds of **representational harms** are generated because of the way GenAI models work. 

Black feminist researchers have played a leading role in highlighting these concerns. 

{: .activity}
![Activity](img/pencilpencil.svg)Read more about how [generative AI models](https://marionwalton.github.io/media-and-society/Copy%20of%204_automation.html) work. 

### Bias in computer vision

Computer vision researchers warned early on that black faces were not even recognised as human, and that  facial recognition did not recognise black women's faces ([Buolamwini & Gebru, 2018](references.html#buolamwini_2018)) 

### Bias in LLMs
As language models grew larger, [Bender, Gebru, et al. (2021)](references.html#bender_2021)  warned that synthetic text generation could reproduce systems of oppression in addition to spreading misinformation, and creating negative environmental and socio-economic impacts. 

These risks related to the way language models rely on training data from undocumented datasets filled with hegemonic viewpoints, and collected with no accountability. 

Gender stereotypes in linguistic data have been shown to be associated with stereotypical viewpoints and implicit judgements ([Lewis, Molly, and Gary Lupyan. 2020](references.html#lewis_2020)). Language models tend to reproduce and perpetuate such stereotypes, and have been associated with unfair discrimination, exclusionary norms, and toxic language, ([Weidinger, Mellor & Rauh, et al., 2021](references.html#weidinger_2021)). They also do not perform as well with less globally dominant languages.

Sources of bias include:
- Structural bias
- Activity bias
- Data (sample/selection) bias
- Confirmation bias

### Gender in marked and unmarked terms

In English, normative gender identities tend to go unmarked, while non-normative gender identities are often explicitly marked, either by the use of a specific gendered term (e.g. in the past “actress” was used for women actors) or by using adjectives (e.g. “woman engineer”, “gay wedding” or “a strong, independent woman”. In such cases, the default or unmarked state (“actor”, “engineer”, "wedding”, "woman") is the normative identity (which usually goes without saying and thus doesn’t need to be stated explicitly e.g. a “\[male\] actor or engineer”, “\[straight\] wedding” or a “\[weak, dependent\] woman”). 

The idea of an unmarked term refers to how the labels for people reflect social biases in the meanings of language. Unmarked terms (actor, nurse) and marked terms (actress, male nurse) name occupations and reflect social inequalities and norms about the gender of people in those occupations. Marginalised social categories tend to be marked (e.g. gay wedding), while dominant perspectives often aren’t specified in the same way (people seldom talk about a “straight wedding”). 

Culturally unmarked qualities are taken for granted and carry meanings which are almost too obvious to state (“that goes without saying"). Choosing to use a marked or unmarked term thus signifies your perspective and social context e.g. if you belong to a Muslim family and are chatting to your family about a cousin’s wedding, you probably wouldn’t mark this identity, since it “goes without saying” in this context. 

# Question
{: .activity}
![Activity](img/pencilpencil.svg)Use concepts from this chapter to discuss some different ways Figure 2 reflects [gender normativity](definitions.html#gender-normativity).

![Wedding photo of a soldier and a model - Generated by Midjourney](img/wedding_photo_midjourney.png "Wedding photo of a soldier and a model - Generated by Midjourney")
Figure 2: Wedding photo of a soldier and a model - Generated by Midjourney

<details markdown="block">
<summary>Click here for answer</summary>

Gender normativity means “adhering to or reinforcing hegemonic, normative standards of masculinity or femininity”. 
Heteronormativity assumes the "normal" relationship is between one man and one woman, embodying conventional gender roles & norms (hegemonic masculinity and femininity) (Barker, 84). Such normativities are kept in place through associations between binary oppositions  e.g. man vs woman, strong vs weak, dominant vs subordinate. 

**Race and Gender**

Midjourney generated weddings of white couples in this example. 

The unmarked identities in the prompt were "model" (all women) and "soldier" (all men). Reflecting gender stereotypes, the unmarked term “soldier” is associated with men, while the unmarked term “model” is associated with women. The women depicted were all examples of hegemonic femininity (brides/models), while the images of men in most cases represented hegemonic masculinity (grooms/soldiers).

**Culture**
Midjourney interpreted the prompt as a wedding scene using the traditional dress for a "white" Western/Christian wedding, rather than representing the wedding costumes of any other tradition. 

**Heteronormativity**

The weddings are all heteronormative weddings between a man (or masculine presenting person) and a woman (or feminine presenting person). 

**Generating poses**

Poses and interaction tend to reflect normative gendering and wedding pics conform to the ritualised subordination of femininity familiar from Goffman's study of gender display in advertising. So for example, notice the differences in height between men and women and exaggerated power differences in many poses. 

**Versions of masculinity**

There are several incongruities linked to outputs where signifiers suggest an inappropriate version of masculinity, since some soldiers are depicted in action on the battlefield, rather than wearing military dress uniform to a wedding ceremony. Bombed buildings, warfare, weapons and a violent version of hyper-masculinity reflect training data which included documentary, film or historical treatment of war. These details are both jarring and amusing in the context of a wedding celebration.

</details>

{: .activity}
![Activity](img/pencilpencil.svg) How does Google's Gemini interpret your photos? Multimodal LLMs like Gemini can be used to describe and access information about pictures. The descriptions and data gleaned in this way can include information about gender and sexuality.  Upload one of your pics here to try it out <https://theyseeyourphotos.com/> .... and then think about (i) whether you would be okay sharing the results of the experiment with other people (ii) how you feel about this kind of information being used by marketers?








